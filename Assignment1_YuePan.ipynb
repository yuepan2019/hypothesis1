{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis1\n",
    "#analyze native speaker students' writing\n",
    "f = open ('nativespeaker.txt','r') #open the text file\n",
    "ns = f.read() #read the text file\n",
    "ns1 = ns.lower() #change all characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = nltk.word_tokenize(ns1) #tokenize the text\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ns = [word for word in tokens if word.isalpha()] #remove punctuations\n",
    "#print(words_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19555\n",
      "3735\n",
      "0.19099974431091793\n"
     ]
    }
   ],
   "source": [
    "tok_ns = len(words_ns)#calculate the number of tokens\n",
    "print(tok_ns)\n",
    "from collections import Counter\n",
    "counts_ns = Counter(words_ns) \n",
    "typ_ns =len(counts_ns)#calculate the number of types\n",
    "print (typ_ns)\n",
    "ttr_ns = typ_ns/tok_ns #calculate the type-token ratio\n",
    "print(ttr_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze non-native speaker students' writing\n",
    "f = open ('nonnativespeaker.txt','r') #open the text file\n",
    "nns = f.read() #read the text file\n",
    "nns1 = nns.lower() #change all characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = nltk.word_tokenize(nns1) #tokenize the text\n",
    "#print(tokens2)\n",
    "words_nns = [word for word in tokens2 if word.isalpha()] #remove punctuations\n",
    "#print(words_nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19306\n",
      "3331\n",
      "0.17253703511861598\n"
     ]
    }
   ],
   "source": [
    "tok_nns = len(words_nns)#calculate the number of tokens\n",
    "print(tok_nns)\n",
    "from collections import Counter\n",
    "counts_nns = Counter(words_nns) \n",
    "typ_nns =len(counts_nns)#calculate the number of types\n",
    "print (typ_nns)\n",
    "ttr_nns = typ_nns/tok_nns #calculate the type-token ratio\n",
    "print(ttr_nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis2\n",
    "#analyze native speaker students' writing\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_ns2 = [w for w in words_ns if not w in stop_words]\n",
    "#Filter out Stop Words such as “the“, “a“, and “is“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alternative', 'become', 'important', 'political', 'however', 'country', 'america', 'people', 'american', 'social', 'culture', 'united', 'states', 'central', 'americans', 'ideology', 'society', 'poverty', 'subject', 'process', 'change', 'result', 'students', 'rosenhan', 'article', 'individual', 'english', 'perception', 'reality', 'cultural', 'language', 'transition', 'democracy', 'spanish', 'gandhi', 'wright', 'gender']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freqDist_ns = FreqDist(words_ns2) #find out frequently used words\n",
    "fre_ns = list(freqDist_ns.keys())#list frequently used words\n",
    "long_words = [w for w in freqDist_ns if len(w) > 5 and freqDist_ns[w] > 15] \n",
    "#words are longer than 5 letters, and occur more than 15 times.\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'important', 'political', 'michael', 'however', 'people', 'social', 'culture', 'states', 'society', 'modern', 'process', 'instead', 'research', 'nature', 'different', 'common', 'individual', 'example', 'information', 'validity', 'another', 'school', 'system', 'children', 'without', 'systems', 'universal', 'parents', 'cultural', 'theory', 'economic', 'market', 'rights', 'resources', 'capital', 'symbolic']\n"
     ]
    }
   ],
   "source": [
    "#analyze non-native speaker students' writing\n",
    "words_nns2 = [w for w in words_nns if not w in stop_words]\n",
    "#Filter out Stop Words such as “the“, “a“, and “is“\n",
    "freqDist_nns = FreqDist(words_nns2) #find out frequently used words\n",
    "fre_nns = list(freqDist_nns.keys())#list frequently used words\n",
    "long_words2 = [w for w in freqDist_ns if len(w) > 5 and freqDist_nns[w] > 15] \n",
    "#words are longer than 5 letters, and occur more than 15 times.\n",
    "print(long_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis3\n",
    "import re\n",
    "s = re.sub('\"(.*?)\"', '',ns1) #replace everything between quotation marks into blanks to ignore quotations\n",
    "#print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "i = re.findall(r\"\\bi\\b\",s)\n",
    "i2 = len(i)\n",
    "print(i2)\n",
    "we = re.findall(r\"\\bwe\\b\",s)\n",
    "we2 = len(we)\n",
    "print(we2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "s2 = re.sub('\"(.*?)\"', '',nns1) #replace everything between quotation marks into blanks to ignore quotations\n",
    "#print (s2)\n",
    "i2 = re.findall(r\"\\bi\\b\",s2)\n",
    "i3 = len(i2)\n",
    "print(i3)\n",
    "we2 = re.findall(r\"\\bwe\\b\",s2)\n",
    "we3 = len(we2)\n",
    "print(we3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
